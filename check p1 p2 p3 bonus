p1: K3s and Vagrant (2 nodes)

From your Ubuntu 22.04 host VM:
cd /path/to/Inception_of_Things/p1

vagrant up

This should:

    Start alrahmouS and alrahmouSW with IPs 192.168.56.110 and 192.168.56.111.

Run setup_server.sh on alrahmouS (K3s server).

Run setup_worker.sh on alrahmouSW (K3s agent joining server).

    ​

Then:
vagrant ssh alrahmouS
kubectl get nodes -o wide

You should see something like:

    Two nodes:

        alrahmouS with role control-plane or master, IP 192.168.56.110.

        alrahmouSW as worker, IP 192.168.56.111.

        ​

Example shape:
NAME         STATUS   ROLES                  AGE   VERSION        INTERNAL-IP
alrahmouS    Ready    control-plane,master   ...   v1.xx.x+k3s1   192.168.56.110
alrahmouSW   Ready    <none>                 ...   v1.xx.x+k3s1   192.168.56.111

You can also check from the worker:
vagrant ssh alrahmouSW
kubectl get nodes -o wide

(it should show the same cluster because kubectl is symlinked to k3s).
*******************************************************************************
p2: K3s + 3 apps + Ingress

From your Ubuntu host VM:
cd /path/to/Inception_of_Things/p2

vagrant up
This should:

    Start alrahmouS with IP 192.168.56.110.

Install K3s server.

Apply /vagrant/confs/apps.yaml (3 apps, app2 with 3 replicas) and /vagrant/confs/ingress.yaml.

Then:

vagrant ssh alrahmouS

kubectl get nodes -o wide
kubectl get all

You should see:

    Node alrahmouS Ready.

    Deployments:

        app1 with 1 pod.

        app2 with 3 pods.

        app3 with 1 pod.

    Services: app1, app2, app3, plus kubernetes.

Example shape:

NAME                         READY   STATUS    RESTARTS   AGE
pod/app1-...                 1/1     Running   0          ...
pod/app2-...                 1/1     Running   0          ...
pod/app2-...                 1/1     Running   0          ...
pod/app2-...                 1/1     Running   0          ...
pod/app3-...                 1/1     Running   0          ...

NAME                 TYPE        CLUSTER-IP      PORT(S)
service/app1         ClusterIP   10.43.x.x       80/TCP
service/app2         ClusterIP   10.43.x.x       80/TCP
service/app3         ClusterIP   10.43.x.x       80/TCP
service/kubernetes   ClusterIP   10.43.0.1       443/TCP

From your host (42 machine or Ubuntu VM), test Ingress routing:
curl -H "Host:app1.com" 192.168.56.110
curl -H "Host:app2.com" 192.168.56.110
curl -H "Host:whatever.com" 192.168.56.110   # should hit app3 (default rule)

First command should return app1 content.

Second: app2 content (one of the 3 replicas).

Third: app3 content, because the Ingress rule without host acts as default.
**********************************************************************
For p3 and bonus, the equivalent “what to run / what you should see” is:
p3: K3d + Argo CD + GitHub

Run on your Ubuntu host VM (not inside Vagrant):
cd /path/to/Inception_of_Things

# 1) Install tools once
p3/scripts/install_tools.sh

# 2) Create k3d cluster + Argo CD + Application
p3/scripts/setup_k3d_argocd.sh

Then check:
kubectl get ns
kubectl get pods -n argocd
kubectl get pods -n dev

You should see:

    Namespaces: argocd, dev.

​

Several argocd-* pods in argocd (Running).

​

One alrahmou-app pod in dev (Running), coming from p3/k8s/dev/deployment.yaml.

    ​

Version‑switch test (GitHub):

    In p3/k8s/dev/deployment.yaml on GitHub, change:
    image: wil42/playground:v1
to
	image: wil42/playground:v2
then git commit && git push.


Wait a bit or trigger sync in Argo CD UI, then:
kubectl get pods -n dev

You should see the old pod terminating and a new one created, proving Argo CD updated from GitHub

**************************************************************************
bonus: GitLab

Assuming p3 cluster is already running and context is set to k3d-k3d-iot:
cd /path/to/Inception_of_Things

# Deploy GitLab in gitlab namespace
bonus/scripts/deploy_gitlab.sh

You should see:

    Namespace gitlab.

​

Several GitLab pods (they may take a while to be Ready).

    ​

Then:

    Use the root password printed by the script and open http://localhost:8080 in a browser to log into GitLab.

    Create a project/repo in GitLab with the same deployment.yaml/service.yaml structure as in GitHub.

​

Create or modify an Argo CD Application to use the GitLab repo URL instead of GitHub, then repeat the v1→v2 image change in GitLab and show pods in dev updating as before.
